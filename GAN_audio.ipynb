{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchaudio torchvision transformers"
      ],
      "metadata": {
        "id": "nt7pXqzaznaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwXEVqdEzc9f"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# 0. IMPORTS & INITIAL SETUP\n",
        "# ==============================================================================\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Audio, display"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive (if using Colab)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctSN1_REzrAQ",
        "outputId": "2a0e0774-1519-4181-b786-607875803327"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 1. DATASET CLASS (Unchanged from your VAE setup)\n",
        "# ==============================================================================\n",
        "\n",
        "class TrainAudioSpectrogramDataset(Dataset):\n",
        "    \"\"\"\n",
        "    This dataset class is reused directly. It loads audio from subfolders,\n",
        "    creates a log-mel-spectrogram, and provides a one-hot encoded label.\n",
        "    This is exactly what our Conditional GAN needs.\n",
        "    \"\"\"\n",
        "    def __init__(self, root_dir, categories, max_frames=512, fraction=1.0):\n",
        "        self.root_dir = root_dir\n",
        "        self.categories = categories\n",
        "        self.max_frames = max_frames\n",
        "        self.file_list = []\n",
        "        self.class_to_idx = {cat: i for i, cat in enumerate(categories)}\n",
        "\n",
        "        for cat_name in self.categories:\n",
        "            cat_dir = os.path.join(root_dir, cat_name)\n",
        "            files_in_cat = [os.path.join(cat_dir, f) for f in os.listdir(cat_dir) if f.endswith(\".wav\")]\n",
        "            num_to_sample = int(len(files_in_cat) * fraction)\n",
        "            sampled_files = random.sample(files_in_cat, num_to_sample)\n",
        "            label_idx = self.class_to_idx[cat_name]\n",
        "            self.file_list.extend([(file_path, label_idx) for file_path in sampled_files])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, label = self.file_list[idx]\n",
        "        wav, sr = torchaudio.load(path)\n",
        "        if wav.size(0) > 1:\n",
        "            wav = wav.mean(dim=0, keepdim=True)\n",
        "\n",
        "        mel_spec = torchaudio.transforms.MelSpectrogram(\n",
        "            sample_rate=sr, n_fft=1024, hop_length=256, n_mels=128\n",
        "        )(wav)\n",
        "        log_spec = torch.log1p(mel_spec)\n",
        "\n",
        "        _, _, n_frames = log_spec.shape\n",
        "        if n_frames < self.max_frames:\n",
        "            pad = self.max_frames - n_frames\n",
        "            log_spec = F.pad(log_spec, (0, pad))\n",
        "        else:\n",
        "            log_spec = log_spec[:, :, :self.max_frames]\n",
        "\n",
        "        label_vec = F.one_hot(torch.tensor(label), num_classes=len(self.categories)).float()\n",
        "        return log_spec, label_vec\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. GAN MODEL DEFINITIONS (GENERATOR & DISCRIMINATOR)\n",
        "# ==============================================================================\n",
        "\n",
        "class CGAN_Generator(nn.Module):\n",
        "    \"\"\" The Forger/Artist \"\"\"\n",
        "    def __init__(self, latent_dim, num_categories, spec_shape=(128, 512)):\n",
        "        super().__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_categories = num_categories\n",
        "        self.spec_shape = spec_shape\n",
        "\n",
        "        # Upsampling architecture\n",
        "        self.fc = nn.Linear(latent_dim + num_categories, 256 * 8 * 32)\n",
        "        self.unflatten_shape = (256, 8, 32) # (channels, H, W)\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1), # -> 16x64\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1), # -> 32x128\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1), # -> 64x256\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1), # -> 128x512\n",
        "            nn.ReLU() # Use ReLU to match the log1p output range [0, inf)\n",
        "        )\n",
        "\n",
        "    def forward(self, z, y):\n",
        "        # Concatenate noise vector (z) and label (y)\n",
        "        h = torch.cat([z, y], dim=1)\n",
        "        h = self.fc(h)\n",
        "        h = h.view(-1, *self.unflatten_shape)\n",
        "        fake_spec = self.net(h)\n",
        "        return fake_spec\n",
        "\n",
        "class CGAN_Discriminator(nn.Module):\n",
        "    \"\"\" The Detective/Critic \"\"\"\n",
        "    def __init__(self, num_categories, spec_shape=(128, 512)):\n",
        "        super().__init__()\n",
        "        self.num_categories = num_categories\n",
        "        self.spec_shape = spec_shape\n",
        "        H, W = spec_shape\n",
        "\n",
        "        # Embedding for the label to match the image dimensions\n",
        "        self.label_embedding = nn.Linear(num_categories, H * W)\n",
        "\n",
        "        # Downsampling architecture\n",
        "        self.net = nn.Sequential(\n",
        "            # Input channel is 2: 1 for spectrogram, 1 for label map\n",
        "            nn.Conv2d(2, 32, kernel_size=4, stride=2, padding=1), # -> 64x256\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1), # -> 32x128\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1), # -> 16x64\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1), # -> 8x32\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            # Final output layer to produce a single logit\n",
        "            nn.Conv2d(256, 1, kernel_size=(8, 32), stride=1, padding=0) # -> 1x1\n",
        "        )\n",
        "\n",
        "    def forward(self, spec, y):\n",
        "        # Create a channel for the label and concatenate it with the spectrogram\n",
        "        label_map = self.label_embedding(y).view(-1, 1, *self.spec_shape)\n",
        "        h = torch.cat([spec, label_map], dim=1)\n",
        "\n",
        "        # Pass through the network\n",
        "        logit = self.net(h)\n",
        "        return logit.view(-1, 1) # Flatten to a single value per item in batch\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. UTILITY FUNCTIONS (GENERATION, SAVING)\n",
        "# ==============================================================================\n",
        "\n",
        "def generate_audio_gan(generator, category_idx, num_samples, device, sample_rate=22050):\n",
        "    generator.eval()\n",
        "    num_categories = generator.num_categories\n",
        "    latent_dim = generator.latent_dim\n",
        "\n",
        "    # Prepare label and noise\n",
        "    y = F.one_hot(torch.tensor([category_idx]), num_classes=num_categories).float().to(device)\n",
        "    z = torch.randn(num_samples, latent_dim, device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        log_spec_gen = generator(z, y)\n",
        "\n",
        "    # Convert spectrogram back to audio (same as VAE)\n",
        "    spec_gen = torch.expm1(log_spec_gen)\n",
        "    spec_gen = spec_gen.squeeze(1)\n",
        "\n",
        "    inverse_mel = torchaudio.transforms.InverseMelScale(\n",
        "        n_stft=1024 // 2 + 1, n_mels=128, sample_rate=sample_rate\n",
        "    ).to(device)\n",
        "    linear_spec = inverse_mel(spec_gen)\n",
        "\n",
        "    griffin = torchaudio.transforms.GriffinLim(\n",
        "        n_fft=1024, hop_length=256, win_length=1024, n_iter=32\n",
        "    ).to(device)\n",
        "\n",
        "    waveform = griffin(linear_spec)\n",
        "    return waveform.cpu()\n",
        "\n",
        "def save_and_play(wav, sample_rate, filename):\n",
        "    if wav.dim() > 2: wav = wav.squeeze(0)\n",
        "    torchaudio.save(filename, wav, sample_rate=sample_rate)\n",
        "    print(f\"Saved to {filename}\")\n",
        "    display(Audio(data=wav.numpy(), rate=sample_rate))\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. GAN TRAINING FUNCTION\n",
        "# ==============================================================================\n",
        "\n",
        "def train_gan(generator, discriminator, dataloader, device, categories, epochs, lr, latent_dim):\n",
        "    # Optimizers for each model\n",
        "    optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "    optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "\n",
        "    # Loss function\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    # Create directories for output\n",
        "    os.makedirs(\"gan_generated_audio\", exist_ok=True)\n",
        "    os.makedirs(\"gan_spectrogram_plots\", exist_ok=True)\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        loop = tqdm(dataloader, desc=f\"Epoch {epoch}/{epochs}\", leave=True)\n",
        "        for real_specs, labels in loop:\n",
        "            real_specs = real_specs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            batch_size = real_specs.size(0)\n",
        "\n",
        "            # Labels for loss calculation\n",
        "            real_labels_tensor = torch.ones(batch_size, 1, device=device)\n",
        "            fake_labels_tensor = torch.zeros(batch_size, 1, device=device)\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Discriminator\n",
        "            # ---------------------\n",
        "            optimizer_D.zero_grad()\n",
        "            real_output = discriminator(real_specs, labels)\n",
        "            loss_D_real = criterion(real_output, real_labels_tensor)\n",
        "\n",
        "            z = torch.randn(batch_size, latent_dim, device=device)\n",
        "            fake_specs = generator(z, labels)\n",
        "\n",
        "            fake_output = discriminator(fake_specs.detach(), labels)\n",
        "            loss_D_fake = criterion(fake_output, fake_labels_tensor)\n",
        "\n",
        "            loss_D = loss_D_real + loss_D_fake\n",
        "            loss_D.backward()\n",
        "            optimizer_D.step()\n",
        "\n",
        "            # -----------------\n",
        "            #  Train Generator\n",
        "            # -----------------\n",
        "            optimizer_G.zero_grad()\n",
        "            output = discriminator(fake_specs, labels)\n",
        "            loss_G = criterion(output, real_labels_tensor)\n",
        "\n",
        "            loss_G.backward()\n",
        "            optimizer_G.step()\n",
        "\n",
        "            loop.set_postfix(loss_D=loss_D.item(), loss_G=loss_G.item())\n",
        "\n",
        "        # --- End of Epoch: Generate and save samples ---\n",
        "        if epoch % 1 == 0:\n",
        "            print(f\"\\n--- Generating Samples for Epoch {epoch} ---\")\n",
        "            generator.eval()\n",
        "\n",
        "            # --- PLOTTING CODE THAT WAS MISSING ---\n",
        "            fig, axes = plt.subplots(1, len(categories), figsize=(4 * len(categories), 4))\n",
        "            if len(categories) == 1: axes = [axes] # Make it iterable\n",
        "\n",
        "            for cat_idx, cat_name in enumerate(categories):\n",
        "                y_cond = F.one_hot(torch.tensor([cat_idx]), num_classes=generator.num_categories).float().to(device)\n",
        "                z_sample = torch.randn(1, generator.latent_dim).to(device)\n",
        "                with torch.no_grad():\n",
        "                    spec_gen_log = generator(z_sample, y_cond)\n",
        "\n",
        "                spec_gen_log_np = spec_gen_log.squeeze().cpu().numpy()\n",
        "                axes[cat_idx].imshow(spec_gen_log_np, aspect='auto', origin='lower', cmap='viridis')\n",
        "                axes[cat_idx].set_title(f'{cat_name} (Epoch {epoch})')\n",
        "                axes[cat_idx].axis('off')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(f'gan_spectrogram_plots/epoch_{epoch:03d}.png')\n",
        "            plt.show()\n",
        "            plt.close(fig) # Close the figure to free up memory\n",
        "            # --- END OF PLOTTING CODE ---\n",
        "\n",
        "            # --- Audio generation (was already here) ---\n",
        "            for cat_idx, cat_name in enumerate(categories):\n",
        "                wav = generate_audio_gan(generator, cat_idx, 1, device)\n",
        "                fname = f\"gan_generated_audio/{cat_name}_ep{epoch}.wav\"\n",
        "                save_and_play(wav, sample_rate=22050, filename=fname)\n",
        "\n",
        "            generator.train() # Set back to training mode\n",
        "            print(\"--- End of Sample Generation ---\\n\")"
      ],
      "metadata": {
        "id": "owRE7CP3zv7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# 5. MAIN EXECUTION BLOCK\n",
        "# ==============================================================================\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # --- Configuration ---\n",
        "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    LATENT_DIM = 100 # Standard for GANs\n",
        "    EPOCHS = 200 # GANs often require more epochs\n",
        "    BATCH_SIZE = 32\n",
        "    LEARNING_RATE = 2e-4 # Common learning rate for GANs with Adam\n",
        "\n",
        "    # --- Paths and Data Setup ---\n",
        "    BASE_PATH = 'drive/MyDrive/organized_dataset/'\n",
        "    TRAIN_PATH = os.path.join(BASE_PATH, 'train')\n",
        "    train_categories = sorted([d for d in os.listdir(TRAIN_PATH) if os.path.isdir(os.path.join(TRAIN_PATH, d))])\n",
        "    NUM_CATEGORIES = len(train_categories)\n",
        "\n",
        "    print(f\"Using device: {DEVICE}\")\n",
        "    print(f\"Found {NUM_CATEGORIES} categories: {train_categories}\")\n",
        "\n",
        "    train_dataset = TrainAudioSpectrogramDataset(\n",
        "        root_dir=TRAIN_PATH, categories=train_categories\n",
        "    )\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "\n",
        "    # --- Initialize Models ---\n",
        "    generator = CGAN_Generator(LATENT_DIM, NUM_CATEGORIES).to(DEVICE)\n",
        "    discriminator = CGAN_Discriminator(NUM_CATEGORIES).to(DEVICE)\n",
        "\n",
        "    # --- Start Training ---\n",
        "    train_gan(\n",
        "        generator=generator,\n",
        "        discriminator=discriminator,\n",
        "        dataloader=train_loader,\n",
        "        device=DEVICE,\n",
        "        categories=train_categories,\n",
        "        epochs=EPOCHS,\n",
        "        lr=LEARNING_RATE,\n",
        "        latent_dim=LATENT_DIM\n",
        "    )"
      ],
      "metadata": {
        "id": "QhfBSly6zzrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3dAYmgsh1QNz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}